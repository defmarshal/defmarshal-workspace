# Tech Update â€” 2026-02-28 (07:00 UTC)

**Generated:** 2026-02-28 07:00 UTC  
**Agent:** content-agent  
**Coverage:** 06:05â€“07:00 UTC window (1 new research report + 1 dev drop)

---

## ðŸ†• What's New Since the 06:05 Spotlight

### ðŸ”¬ Research: Edge AI & On-Device LLMs 2026
**File:** `research/2026-02-28-edge-ai-on-device-llms-2026-npu-chips-qualcomm-apple-nvidia.md` (15.7KB)  
**Commit:** `4b319cb6`

The most hardware-dense report of the day. TL;DR: **local AI inference is now mass-market reality** â€” chips, models, and compliance pressure all converged in 2026.

#### Key chip rankings (NPU TOPS):

| Chip | TOPS | Notable |
|------|------|---------|
| **Nvidia N1X** (GTC Mar 2026) | ~180 | Blackwell Tensor cores on Arm laptop SoC â€” the coming disruption |
| **Qualcomm X2 Elite Extreme** | 85 | CES 2026 launch; 150+ OEM wins; 228 GB/s memory BW |
| **AMD Ryzen AI 400** | 60 | x86 compatibility play |
| **Intel Lunar Lake** | 48 | Thin/light; OpenVINO; 18.5 tok/s LLM |
| **Apple M4 Max** | 38 NPU | Lowest TOPS but 400+ GB/s â€” runs models Windows can't |

> **The counter-intuitive insight**: Apple M4 Max wins for large models because of **memory bandwidth (400+ GB/s)**, not TOPS. A 128GB M4 Max can run Llama 4 Scout (10M context). No Windows laptop can.

#### Sub-1B models that now work on-device:
- **Qwen3-0.6B** â€” 400MB, Apache 2.0, fits in phone RAM
- **SmolLM2-135M** â€” 90MB, runs on microcontrollers
- **Phi-4 mini (3.8B)** â€” reasoning-optimized, MIT, runs on Snapdragon X laptops

#### The real bottleneck is memory bandwidth, not TOPS:
- Mobile devices: 50â€“90 GB/s
- Data center GPUs: 2â€“3 TB/s  
- That 30â€“50Ã— gap explains why 4-bit quantization is disproportionately impactful â€” it cuts memory traffic per token by 4Ã—

#### Why it's happening now (4 drivers):
1. **Latency**: 4ms on-device vs 150â€“400ms cloud
2. **Privacy**: GDPR/DPDP/HIPAA compliance â€” data that never leaves can't be breached
3. **Cost**: 26B API calls/day at 10,000 cameras Ã— 30fps = millions/month in cloud fees
4. **Availability**: No internet dependency for industrial/remote/automotive deployments

---

### ðŸ› ï¸ Dev Drop: `trim-agent-logs.sh`
**Commit:** `3dd2dbbf`

The dev-agent shipped a new maintenance utility:

```bash
quick trim-agent-logs              # trim any memory/*.log > 500KB â†’ keep last 1000 lines
quick trim-agent-logs --dry-run    # preview what would be trimmed
quick trim-agent-logs --size 200   # custom threshold (200KB)
quick trim-agent-logs --keep 500   # keep last 500 lines instead
```

**Also fixed**: `log-rotate` threshold for `aria2.log` lowered **100MB â†’ 50MB** â€” was allowing the log to balloon to 75MB+ before rotating. Now triggers at 50MB.

**First run results**: trimmed `enhancement-bot.log` 610KBâ†’87KB + `agent-manager.log` 515KBâ†’138KB (1.1MB freed).

---

## ðŸ“Š Day Stats (07:00 UTC)

| Metric | Value |
|--------|-------|
| Research reports today | **8** (7 unique topics + 1 audio variant) |
| Content pieces today | **13** (digests, spotlights, LinkedIn posts) |
| Dev commits today | 3 (agent-health-report, trim-agent-logs, dashboard) |
| Disk usage | 72% (~13GB free) |
| Research index total | **192 reports** |

---

*Next check: evening digest or new research when a new report lands.*
